{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# Load the video file\n",
    "video_path = '../videos/neso.mp4'\n",
    "video_clip = VideoFileClip(video_path)\n",
    "\n",
    "# Extract the audio from the video\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Save the audio to a file\n",
    "audio_path = 'audio.wav'\n",
    "audio_clip.write_audiofile(audio_path)\n",
    "\n",
    "# Close the clips\n",
    "video_clip.close()\n",
    "audio_clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "recognition request failed: Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/speech_recognition/__init__.py:708\u001b[0m, in \u001b[0;36mRecognizer.recognize_google\u001b[0;34m(self, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     response \u001b[39m=\u001b[39m urlopen(request, timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moperation_timeout)\n\u001b[1;32m    709\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    641\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    643\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m             whole_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m text\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m whole_text\n\u001b[0;32m---> 38\u001b[0m transcribe_audio(\u001b[39m\"\u001b[39;49m\u001b[39maudio.wav\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m sr\u001b[39m.\u001b[39mAudioFile(path) \u001b[39mas\u001b[39;00m source:\n\u001b[1;32m     10\u001b[0m     audio_listened \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrecord(source)\n\u001b[0;32m---> 11\u001b[0m     text \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mrecognize_google(audio_listened)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/speech_recognition/__init__.py:710\u001b[0m, in \u001b[0;36mRecognizer.recognize_google\u001b[0;34m(self, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[1;32m    708\u001b[0m     response \u001b[39m=\u001b[39m urlopen(request, timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation_timeout)\n\u001b[1;32m    709\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m RequestError(\u001b[39m\"\u001b[39m\u001b[39mrecognition request failed: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e\u001b[39m.\u001b[39mreason))\n\u001b[1;32m    711\u001b[0m \u001b[39mexcept\u001b[39;00m URLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    712\u001b[0m     \u001b[39mraise\u001b[39;00m RequestError(\u001b[39m\"\u001b[39m\u001b[39mrecognition connection failed: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e\u001b[39m.\u001b[39mreason))\n",
      "\u001b[0;31mRequestError\u001b[0m: recognition request failed: Bad Request"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "def transcribe_audio(path):\n",
    "    with sr.AudioFile(path) as source:\n",
    "        audio_listened = r.record(source)\n",
    "        text = r.recognize_google(audio_listened)\n",
    "    return text\n",
    "\n",
    "def get_large_audio_transcription_on_silence(path):\n",
    "    sound = AudioSegment.from_file(path)  \n",
    "    chunks = split_on_silence(sound,\n",
    "        min_silence_len = 500,\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        try:\n",
    "            text = transcribe_audio(chunk_filename)\n",
    "        except sr.UnknownValueError as e:\n",
    "            print(\"Error:\", str(e))\n",
    "        else:\n",
    "            text = f\"{text.capitalize()}. \"\n",
    "            print(chunk_filename, \":\", text)\n",
    "            whole_text += text\n",
    "    return whole_text\n",
    "\n",
    "transcribe_audio(\"audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in /home/sanatan/.local/lib/python3.8/site-packages (3.10.0)\n",
      "Requirement already satisfied: pydub in /home/sanatan/.local/lib/python3.8/site-packages (0.25.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/sanatan/.local/lib/python3.8/site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sanatan/.local/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install SpeechRecognition pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'foreign', 'start': 0.96, 'duration': 3.0}, {'text': 'we have understood how to update list', 'start': 7.04, 'duration': 6.7}, {'text': 'items in Python now in this presentation', 'start': 10.559, 'duration': 5.881}, {'text': 'we will understand how to remove list', 'start': 13.74, 'duration': 5.76}, {'text': \"items so without any further delay let's\", 'start': 16.44, 'duration': 4.62}, {'text': 'get started', 'start': 19.5, 'duration': 3.96}, {'text': 'the first topic of this presentation is', 'start': 21.06, 'duration': 5.04}, {'text': 'remove an item using the remove method', 'start': 23.46, 'duration': 6.18}, {'text': 'the second topic is remove an item using', 'start': 26.1, 'duration': 6.36}, {'text': 'the pop method the third topic is remove', 'start': 29.64, 'duration': 5.7}, {'text': 'an item using the Dell keyword and the', 'start': 32.46, 'duration': 5.52}, {'text': 'fourth topic is clear the list using the', 'start': 35.34, 'duration': 5.219}, {'text': \"clear method let's start with the first\", 'start': 37.98, 'duration': 4.98}, {'text': 'topic that is remove an item using the', 'start': 40.559, 'duration': 4.621}, {'text': 'remove method', 'start': 42.96, 'duration': 5.64}, {'text': 'remove method removes the specified item', 'start': 45.18, 'duration': 7.199}, {'text': 'so if we pass an item to this remove', 'start': 48.6, 'duration': 6.9}, {'text': 'method it will remove that item from the', 'start': 52.379, 'duration': 5.461}, {'text': \"list for example let's open the command\", 'start': 55.5, 'duration': 4.26}, {'text': 'prompt and activate the python', 'start': 57.84, 'duration': 4.379}, {'text': \"interactive shell now let's type this\", 'start': 59.76, 'duration': 5.7}, {'text': 'command Li equal to John Mike David', 'start': 62.219, 'duration': 5.4}, {'text': \"let's say we have this list which\", 'start': 65.46, 'duration': 5.1}, {'text': 'consists of three items John Mike and', 'start': 67.619, 'duration': 3.781}, {'text': 'David', 'start': 70.56, 'duration': 3.419}, {'text': \"let's say we want to remove this item\", 'start': 71.4, 'duration': 5.579}, {'text': 'John from this list we can pass this', 'start': 73.979, 'duration': 5.401}, {'text': 'item to remove method in order to remove', 'start': 76.979, 'duration': 5.221}, {'text': \"this item from this list so let's hit\", 'start': 79.38, 'duration': 6.599}, {'text': 'enter and type Li dot remove John with', 'start': 82.2, 'duration': 5.7}, {'text': 'this we would be able to remove John', 'start': 85.979, 'duration': 4.261}, {'text': 'from this list in order to check this', 'start': 87.9, 'duration': 5.1}, {'text': \"let's hit enter and type Li and again\", 'start': 90.24, 'duration': 5.699}, {'text': 'hit enter we will get this list where', 'start': 93.0, 'duration': 4.619}, {'text': 'John is not available', 'start': 95.939, 'duration': 4.021}, {'text': 'so I hope with this it is clear how to', 'start': 97.619, 'duration': 4.801}, {'text': 'remove an item using the remove method', 'start': 99.96, 'duration': 5.339}, {'text': \"now let's move on to the next topic that\", 'start': 102.42, 'duration': 6.18}, {'text': 'is remove an item using the pop method', 'start': 105.299, 'duration': 6.18}, {'text': 'the pop method is used to remove an item', 'start': 108.6, 'duration': 6.059}, {'text': 'at the specified index so in place of', 'start': 111.479, 'duration': 6.18}, {'text': 'passing the item we can pass the index', 'start': 114.659, 'duration': 5.701}, {'text': 'of that item to the pop method in order', 'start': 117.659, 'duration': 6.361}, {'text': 'to remove that item for example we will', 'start': 120.36, 'duration': 5.7}, {'text': 'consider the same list with three items', 'start': 124.02, 'duration': 4.62}, {'text': \"John Mike and David now let's say we\", 'start': 126.06, 'duration': 5.28}, {'text': 'want to remove this item mic from this', 'start': 128.64, 'duration': 5.459}, {'text': 'list we know that the index of this item', 'start': 131.34, 'duration': 5.22}, {'text': 'is one so we just need to pass the index', 'start': 134.099, 'duration': 5.701}, {'text': 'of this item to this pop method we can', 'start': 136.56, 'duration': 7.02}, {'text': 'do this by typing Li dot pop one', 'start': 139.8, 'duration': 6.54}, {'text': 'with this we can remove this item from', 'start': 143.58, 'duration': 4.799}, {'text': 'this list but there is one interesting', 'start': 146.34, 'duration': 4.56}, {'text': 'thing that will happen as soon as we hit', 'start': 148.379, 'duration': 6.121}, {'text': 'enter we will get mic as the result why', 'start': 150.9, 'duration': 6.059}, {'text': 'are we getting Mike here because pop', 'start': 154.5, 'duration': 5.76}, {'text': 'method not only removes the item but it', 'start': 156.959, 'duration': 6.541}, {'text': 'also Returns the deleted item so we can', 'start': 160.26, 'duration': 5.88}, {'text': 'use that later in our code we can store', 'start': 163.5, 'duration': 5.34}, {'text': 'this item in some variable and we can', 'start': 166.14, 'duration': 5.28}, {'text': 'use this item in our code if we want to', 'start': 168.84, 'duration': 5.82}, {'text': 'so pop method not only deletes an item', 'start': 171.42, 'duration': 6.42}, {'text': 'it also Returns the deleted item', 'start': 174.66, 'duration': 5.88}, {'text': 'so whenever you want to use the deleted', 'start': 177.84, 'duration': 5.1}, {'text': 'item in your code use the pop method', 'start': 180.54, 'duration': 4.86}, {'text': 'otherwise you can use the remove method', 'start': 182.94, 'duration': 4.379}, {'text': 'so this is the difference between remove', 'start': 185.4, 'duration': 5.1}, {'text': \"method and pop method now let's check\", 'start': 187.319, 'duration': 5.401}, {'text': 'whether this item is deleted from this', 'start': 190.5, 'duration': 5.22}, {'text': 'list or not for this we will type Li and', 'start': 192.72, 'duration': 5.82}, {'text': 'we will hit enter we will get John and', 'start': 195.72, 'duration': 5.28}, {'text': 'David in the list but we do not have mic', 'start': 198.54, 'duration': 3.839}, {'text': 'in this list', 'start': 201.0, 'duration': 3.36}, {'text': 'now there is one more interesting point', 'start': 202.379, 'duration': 4.561}, {'text': 'I would like to mention', 'start': 204.36, 'duration': 5.459}, {'text': 'if the index of the item is not', 'start': 206.94, 'duration': 5.28}, {'text': 'specified then the last item will be', 'start': 209.819, 'duration': 4.801}, {'text': \"deleted so let's say that we won't pass\", 'start': 212.22, 'duration': 5.159}, {'text': 'any index to this pop method in that', 'start': 214.62, 'duration': 6.179}, {'text': 'case the pop method will remove the last', 'start': 217.379, 'duration': 7.08}, {'text': 'item of the list in this example David', 'start': 220.799, 'duration': 6.481}, {'text': 'will be deleted from the list so this is', 'start': 224.459, 'duration': 5.821}, {'text': \"how pop method works now let's move on\", 'start': 227.28, 'duration': 6.0}, {'text': 'to the next topic that is remove an item', 'start': 230.28, 'duration': 5.76}, {'text': 'using the Dell keyword', 'start': 233.28, 'duration': 5.459}, {'text': 'Dell keyword can also be used to remove', 'start': 236.04, 'duration': 6.66}, {'text': 'an item at the specified index', 'start': 238.739, 'duration': 6.72}, {'text': 'so it removes an item at the specified', 'start': 242.7, 'duration': 6.0}, {'text': \"index for example let's open our Command\", 'start': 245.459, 'duration': 5.101}, {'text': \"Prompt again let's consider the same\", 'start': 248.7, 'duration': 4.38}, {'text': 'example list now this time we are', 'start': 250.56, 'duration': 4.92}, {'text': 'interested in deleting the first item of', 'start': 253.08, 'duration': 5.159}, {'text': 'this list using the Dell keyword so we', 'start': 255.48, 'duration': 6.539}, {'text': 'just need to type Del Li within brackets', 'start': 258.239, 'duration': 7.381}, {'text': '0. we need to specify the index of the', 'start': 262.019, 'duration': 5.821}, {'text': 'item which we want to delete and the', 'start': 265.62, 'duration': 4.68}, {'text': 'first thing that we need to type is Del', 'start': 267.84, 'duration': 5.7}, {'text': \"so now let's hit enter and type Li to\", 'start': 270.3, 'duration': 5.28}, {'text': 'check whether the list is updated or not', 'start': 273.54, 'duration': 4.56}, {'text': \"let's hit enter again we will get this\", 'start': 275.58, 'duration': 5.339}, {'text': 'list where John is not available', 'start': 278.1, 'duration': 4.86}, {'text': 'so with this we have learned how to', 'start': 280.919, 'duration': 4.041}, {'text': 'remove an item using the Dell keyboard', 'start': 282.96, 'duration': 5.22}, {'text': 'not only that Dell keyword is also', 'start': 284.96, 'duration': 6.58}, {'text': 'useful in removing the complete list so', 'start': 288.18, 'duration': 6.12}, {'text': 'we can remove the entire list also with', 'start': 291.54, 'duration': 5.939}, {'text': \"the Dell keyword how let's open our\", 'start': 294.3, 'duration': 5.04}, {'text': \"Command Prompt again let's say we want\", 'start': 297.479, 'duration': 4.921}, {'text': 'to delete this entire list so we can', 'start': 299.34, 'duration': 6.84}, {'text': \"type Del Li that's it so we just have to\", 'start': 302.4, 'duration': 6.6}, {'text': 'specify the name of the list after', 'start': 306.18, 'duration': 6.0}, {'text': \"hitting enter our list is deleted let's\", 'start': 309.0, 'duration': 5.22}, {'text': 'type Li to check whether the list is', 'start': 312.18, 'duration': 4.44}, {'text': 'deleted or not as soon as we hit enter', 'start': 314.22, 'duration': 5.52}, {'text': 'we will get name error from The', 'start': 316.62, 'duration': 6.66}, {'text': 'Interpreter which says name Li is not', 'start': 319.74, 'duration': 4.86}, {'text': 'defined', 'start': 323.28, 'duration': 4.62}, {'text': 'this clearly indicates that Li does not', 'start': 324.6, 'duration': 6.12}, {'text': 'exist and this means that the list is', 'start': 327.9, 'duration': 4.56}, {'text': 'completely deleted', 'start': 330.72, 'duration': 4.8}, {'text': 'so Dell keyword is useful when we want', 'start': 332.46, 'duration': 6.0}, {'text': 'to delete the entire list', 'start': 335.52, 'duration': 5.399}, {'text': \"now let's move on to the next topic that\", 'start': 338.46, 'duration': 5.64}, {'text': 'is clear the list using the clear method', 'start': 340.919, 'duration': 5.881}, {'text': \"let's say that the requirement is to not\", 'start': 344.1, 'duration': 6.06}, {'text': 'delete the entire list but to clear the', 'start': 346.8, 'duration': 6.0}, {'text': 'list or empty the list then we can use', 'start': 350.16, 'duration': 4.92}, {'text': 'the clear method for this purpose', 'start': 352.8, 'duration': 6.899}, {'text': 'so clear method can empty the list for', 'start': 355.08, 'duration': 6.54}, {'text': \"example let's open our Command Prompt\", 'start': 359.699, 'duration': 4.5}, {'text': 'again we will consider the same list and', 'start': 361.62, 'duration': 4.68}, {'text': \"now let's say we want to empty this list\", 'start': 364.199, 'duration': 5.461}, {'text': 'then we can type Li dot clear', 'start': 366.3, 'duration': 6.42}, {'text': \"now let's hit enter and type Li and then\", 'start': 369.66, 'duration': 6.24}, {'text': \"again let's hit enter we will get an\", 'start': 372.72, 'duration': 5.699}, {'text': 'empty list we will not get error this', 'start': 375.9, 'duration': 4.859}, {'text': 'time because the list is not completely', 'start': 378.419, 'duration': 5.4}, {'text': 'deleted we still have the list but this', 'start': 380.759, 'duration': 5.94}, {'text': 'time the list is empty so clear method', 'start': 383.819, 'duration': 5.281}, {'text': 'is used to remove the items of the list', 'start': 386.699, 'duration': 5.94}, {'text': 'it will not remove the entire list so', 'start': 389.1, 'duration': 5.819}, {'text': 'this is the difference between Dell and', 'start': 392.639, 'duration': 3.241}, {'text': 'clear', 'start': 394.919, 'duration': 3.601}, {'text': 'so I hope with this the concept of clear', 'start': 395.88, 'duration': 4.8}, {'text': 'method is also clear', 'start': 398.52, 'duration': 4.26}, {'text': 'we are done with all the topics of this', 'start': 400.68, 'duration': 4.2}, {'text': 'presentation we have learned how to', 'start': 402.78, 'duration': 4.32}, {'text': 'remove an item using the remove method', 'start': 404.88, 'duration': 4.319}, {'text': 'we have learned how to remove an item', 'start': 407.1, 'duration': 4.2}, {'text': 'using the pop method we have seen the', 'start': 409.199, 'duration': 3.901}, {'text': 'difference between the two the', 'start': 411.3, 'duration': 4.019}, {'text': 'difference lies in the fact that pop', 'start': 413.1, 'duration': 5.099}, {'text': 'method not only deletes an item but it', 'start': 415.319, 'duration': 5.521}, {'text': 'also Returns the deleted item on the', 'start': 418.199, 'duration': 4.681}, {'text': 'other hand remove method just removes', 'start': 420.84, 'duration': 4.32}, {'text': 'the item from the list we have learned', 'start': 422.88, 'duration': 4.2}, {'text': 'how to remove an item using the Dell', 'start': 425.16, 'duration': 3.36}, {'text': 'keyword also', 'start': 427.08, 'duration': 3.6}, {'text': 'if you want to delete the entire list', 'start': 428.52, 'duration': 4.679}, {'text': 'then Dell keyword is useful and we have', 'start': 430.68, 'duration': 4.68}, {'text': 'also learned how to clear the list or', 'start': 433.199, 'duration': 4.681}, {'text': 'empty the list using the clear method', 'start': 435.36, 'duration': 5.04}, {'text': 'so with this I hope the concepts of this', 'start': 437.88, 'duration': 4.98}, {'text': 'presentation is completely clear', 'start': 440.4, 'duration': 4.799}, {'text': 'okay friends this is it for now thank', 'start': 442.86, 'duration': 4.38}, {'text': 'you for watching this presentation I', 'start': 445.199, 'duration': 3.981}, {'text': 'will see you in the next one', 'start': 447.24, 'duration': 2.12}, {'text': '[Music]', 'start': 449.18, 'duration': 2.35}, {'text': '[Applause]', 'start': 449.36, 'duration': 12.36}, {'text': '[Music]', 'start': 451.53, 'duration': 10.19}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" foreign we have understood how to update list items in Python now in this presentation we will understand how to remove list items so without any further delay let's get started the first topic of this presentation is remove an item using the remove method the second topic is remove an item using the pop method the third topic is remove an item using the Dell keyword and the fourth topic is clear the list using the clear method let's start with the first topic that is remove an item using the remove method remove method removes the specified item so if we pass an item to this remove method it will remove that item from the list for example let's open the command prompt and activate the python interactive shell now let's type this command Li equal to John Mike David let's say we have this list which consists of three items John Mike and David let's say we want to remove this item John from this list we can pass this item to remove method in order to remove this item from this list so let's hit enter and type Li dot remove John with this we would be able to remove John from this list in order to check this let's hit enter and type Li and again hit enter we will get this list where John is not available so I hope with this it is clear how to remove an item using the remove method now let's move on to the next topic that is remove an item using the pop method the pop method is used to remove an item at the specified index so in place of passing the item we can pass the index of that item to the pop method in order to remove that item for example we will consider the same list with three items John Mike and David now let's say we want to remove this item mic from this list we know that the index of this item is one so we just need to pass the index of this item to this pop method we can do this by typing Li dot pop one with this we can remove this item from this list but there is one interesting thing that will happen as soon as we hit enter we will get mic as the result why are we getting Mike here because pop method not only removes the item but it also Returns the deleted item so we can use that later in our code we can store this item in some variable and we can use this item in our code if we want to so pop method not only deletes an item it also Returns the deleted item so whenever you want to use the deleted item in your code use the pop method otherwise you can use the remove method so this is the difference between remove method and pop method now let's check whether this item is deleted from this list or not for this we will type Li and we will hit enter we will get John and David in the list but we do not have mic in this list now there is one more interesting point I would like to mention if the index of the item is not specified then the last item will be deleted so let's say that we won't pass any index to this pop method in that case the pop method will remove the last item of the list in this example David will be deleted from the list so this is how pop method works now let's move on to the next topic that is remove an item using the Dell keyword Dell keyword can also be used to remove an item at the specified index so it removes an item at the specified index for example let's open our Command Prompt again let's consider the same example list now this time we are interested in deleting the first item of this list using the Dell keyword so we just need to type Del Li within brackets 0. we need to specify the index of the item which we want to delete and the first thing that we need to type is Del so now let's hit enter and type Li to check whether the list is updated or not let's hit enter again we will get this list where John is not available so with this we have learned how to remove an item using the Dell keyboard not only that Dell keyword is also useful in removing the complete list so we can remove the entire list also with the Dell keyword how let's open our Command Prompt again let's say we want to delete this entire list so we can type Del Li that's it so we just have to specify the name of the list after hitting enter our list is deleted let's type Li to check whether the list is deleted or not as soon as we hit enter we will get name error from The Interpreter which says name Li is not defined this clearly indicates that Li does not exist and this means that the list is completely deleted so Dell keyword is useful when we want to delete the entire list now let's move on to the next topic that is clear the list using the clear method let's say that the requirement is to not delete the entire list but to clear the list or empty the list then we can use the clear method for this purpose so clear method can empty the list for example let's open our Command Prompt again we will consider the same list and now let's say we want to empty this list then we can type Li dot clear now let's hit enter and type Li and then again let's hit enter we will get an empty list we will not get error this time because the list is not completely deleted we still have the list but this time the list is empty so clear method is used to remove the items of the list it will not remove the entire list so this is the difference between Dell and clear so I hope with this the concept of clear method is also clear we are done with all the topics of this presentation we have learned how to remove an item using the remove method we have learned how to remove an item using the pop method we have seen the difference between the two the difference lies in the fact that pop method not only deletes an item but it also Returns the deleted item on the other hand remove method just removes the item from the list we have learned how to remove an item using the Dell keyword also if you want to delete the entire list then Dell keyword is useful and we have also learned how to clear the list or empty the list using the clear method so with this I hope the concepts of this presentation is completely clear okay friends this is it for now thank you for watching this presentation I will see you in the next one [Music] [Applause] [Music]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Retrieve the transcript for the video\n",
    "video_id = 'f8YtlYffg7g'\n",
    "transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "print(transcript_list)\n",
    "\n",
    "# Convert the transcript to text\n",
    "transcript_text = ''\n",
    "for line in transcript_list:\n",
    "    transcript_text += ' ' + line['text']\n",
    "transcript_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sanatan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"foreign understood update list items Python presentation understand remove list items without delay let 's get started first topic presentation remove item using remove method second topic remove item using pop method third topic remove item using Dell keyword fourth topic clear list using clear method let 's start first topic remove item using remove method remove method removes specified item pass item remove method remove item list example let 's open command prompt activate python interactive shell let 's type command Li equal John Mike David let 's say list consists three items John Mike David let 's say want remove item John list pass item remove method order remove item list let 's hit enter type Li dot remove John would able remove John list order check let 's hit enter type Li hit enter get list John available hope clear remove item using remove method let 's move next topic remove item using pop method pop method used remove item specified index place passing item pass index item pop method order remove item example consider list three items John Mike David let 's say want remove item mic list know index item one need pass index item pop method typing Li dot pop one remove item list one interesting thing happen soon hit enter get mic result getting Mike pop method removes item also Returns deleted item use later code store item variable use item code want pop method deletes item also Returns deleted item whenever want use deleted item code use pop method otherwise use remove method difference remove method pop method let 's check whether item deleted list type Li hit enter get John David list mic list one interesting point would like mention index item specified last item deleted let 's say wo n't pass index pop method case pop method remove last item list example David deleted list pop method works let 's move next topic remove item using Dell keyword Dell keyword also used remove item specified index removes item specified index example let 's open Command Prompt let 's consider example list time interested deleting first item list using Dell keyword need type Del Li within brackets 0. need specify index item want delete first thing need type Del let 's hit enter type Li check whether list updated let 's hit enter get list John available learned remove item using Dell keyboard Dell keyword also useful removing complete list remove entire list also Dell keyword let 's open Command Prompt let 's say want delete entire list type Del Li 's specify name list hitting enter list deleted let 's type Li check whether list deleted soon hit enter get name error Interpreter says name Li defined clearly indicates Li exist means list completely deleted Dell keyword useful want delete entire list let 's move next topic clear list using clear method let 's say requirement delete entire list clear list empty list use clear method purpose clear method empty list example let 's open Command Prompt consider list let 's say want empty list type Li dot clear let 's hit enter type Li let 's hit enter get empty list get error time list completely deleted still list time list empty clear method used remove items list remove entire list difference Dell clear hope concept clear method also clear done topics presentation learned remove item using remove method learned remove item using pop method seen difference two difference lies fact pop method deletes item also Returns deleted item hand remove method removes item list learned remove item using Dell keyword also want delete entire list Dell keyword useful also learned clear list empty list using clear method hope concepts presentation completely clear okay friends thank watching presentation see next one [ Music ] [ Applause ] [ Music ]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokens = word_tokenize(transcript_text)\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
    "\n",
    "text = ' '.join(filtered_tokens)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('foreign', 'understood', 'update', 'list')\n",
      "('understood', 'update', 'list', 'items')\n",
      "('update', 'list', 'items', 'Python')\n",
      "('list', 'items', 'Python', 'presentation')\n",
      "('items', 'Python', 'presentation', 'understand')\n",
      "('Python', 'presentation', 'understand', 'remove')\n",
      "('presentation', 'understand', 'remove', 'list')\n",
      "('understand', 'remove', 'list', 'items')\n",
      "('remove', 'list', 'items', 'without')\n",
      "('list', 'items', 'without', 'delay')\n",
      "('items', 'without', 'delay', 'let')\n",
      "('without', 'delay', 'let', \"'s\")\n",
      "('delay', 'let', \"'s\", 'get')\n",
      "('let', \"'s\", 'get', 'started')\n",
      "(\"'s\", 'get', 'started', 'first')\n",
      "('get', 'started', 'first', 'topic')\n",
      "('started', 'first', 'topic', 'presentation')\n",
      "('first', 'topic', 'presentation', 'remove')\n",
      "('topic', 'presentation', 'remove', 'item')\n",
      "('presentation', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'remove')\n",
      "('item', 'using', 'remove', 'method')\n",
      "('using', 'remove', 'method', 'second')\n",
      "('remove', 'method', 'second', 'topic')\n",
      "('method', 'second', 'topic', 'remove')\n",
      "('second', 'topic', 'remove', 'item')\n",
      "('topic', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'pop')\n",
      "('item', 'using', 'pop', 'method')\n",
      "('using', 'pop', 'method', 'third')\n",
      "('pop', 'method', 'third', 'topic')\n",
      "('method', 'third', 'topic', 'remove')\n",
      "('third', 'topic', 'remove', 'item')\n",
      "('topic', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'Dell')\n",
      "('item', 'using', 'Dell', 'keyword')\n",
      "('using', 'Dell', 'keyword', 'fourth')\n",
      "('Dell', 'keyword', 'fourth', 'topic')\n",
      "('keyword', 'fourth', 'topic', 'clear')\n",
      "('fourth', 'topic', 'clear', 'list')\n",
      "('topic', 'clear', 'list', 'using')\n",
      "('clear', 'list', 'using', 'clear')\n",
      "('list', 'using', 'clear', 'method')\n",
      "('using', 'clear', 'method', 'let')\n",
      "('clear', 'method', 'let', \"'s\")\n",
      "('method', 'let', \"'s\", 'start')\n",
      "('let', \"'s\", 'start', 'first')\n",
      "(\"'s\", 'start', 'first', 'topic')\n",
      "('start', 'first', 'topic', 'remove')\n",
      "('first', 'topic', 'remove', 'item')\n",
      "('topic', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'remove')\n",
      "('item', 'using', 'remove', 'method')\n",
      "('using', 'remove', 'method', 'remove')\n",
      "('remove', 'method', 'remove', 'method')\n",
      "('method', 'remove', 'method', 'removes')\n",
      "('remove', 'method', 'removes', 'specified')\n",
      "('method', 'removes', 'specified', 'item')\n",
      "('removes', 'specified', 'item', 'pass')\n",
      "('specified', 'item', 'pass', 'item')\n",
      "('item', 'pass', 'item', 'remove')\n",
      "('pass', 'item', 'remove', 'method')\n",
      "('item', 'remove', 'method', 'remove')\n",
      "('remove', 'method', 'remove', 'item')\n",
      "('method', 'remove', 'item', 'list')\n",
      "('remove', 'item', 'list', 'example')\n",
      "('item', 'list', 'example', 'let')\n",
      "('list', 'example', 'let', \"'s\")\n",
      "('example', 'let', \"'s\", 'open')\n",
      "('let', \"'s\", 'open', 'command')\n",
      "(\"'s\", 'open', 'command', 'prompt')\n",
      "('open', 'command', 'prompt', 'activate')\n",
      "('command', 'prompt', 'activate', 'python')\n",
      "('prompt', 'activate', 'python', 'interactive')\n",
      "('activate', 'python', 'interactive', 'shell')\n",
      "('python', 'interactive', 'shell', 'let')\n",
      "('interactive', 'shell', 'let', \"'s\")\n",
      "('shell', 'let', \"'s\", 'type')\n",
      "('let', \"'s\", 'type', 'command')\n",
      "(\"'s\", 'type', 'command', 'Li')\n",
      "('type', 'command', 'Li', 'equal')\n",
      "('command', 'Li', 'equal', 'John')\n",
      "('Li', 'equal', 'John', 'Mike')\n",
      "('equal', 'John', 'Mike', 'David')\n",
      "('John', 'Mike', 'David', 'let')\n",
      "('Mike', 'David', 'let', \"'s\")\n",
      "('David', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'list')\n",
      "(\"'s\", 'say', 'list', 'consists')\n",
      "('say', 'list', 'consists', 'three')\n",
      "('list', 'consists', 'three', 'items')\n",
      "('consists', 'three', 'items', 'John')\n",
      "('three', 'items', 'John', 'Mike')\n",
      "('items', 'John', 'Mike', 'David')\n",
      "('John', 'Mike', 'David', 'let')\n",
      "('Mike', 'David', 'let', \"'s\")\n",
      "('David', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'want')\n",
      "(\"'s\", 'say', 'want', 'remove')\n",
      "('say', 'want', 'remove', 'item')\n",
      "('want', 'remove', 'item', 'John')\n",
      "('remove', 'item', 'John', 'list')\n",
      "('item', 'John', 'list', 'pass')\n",
      "('John', 'list', 'pass', 'item')\n",
      "('list', 'pass', 'item', 'remove')\n",
      "('pass', 'item', 'remove', 'method')\n",
      "('item', 'remove', 'method', 'order')\n",
      "('remove', 'method', 'order', 'remove')\n",
      "('method', 'order', 'remove', 'item')\n",
      "('order', 'remove', 'item', 'list')\n",
      "('remove', 'item', 'list', 'let')\n",
      "('item', 'list', 'let', \"'s\")\n",
      "('list', 'let', \"'s\", 'hit')\n",
      "('let', \"'s\", 'hit', 'enter')\n",
      "(\"'s\", 'hit', 'enter', 'type')\n",
      "('hit', 'enter', 'type', 'Li')\n",
      "('enter', 'type', 'Li', 'dot')\n",
      "('type', 'Li', 'dot', 'remove')\n",
      "('Li', 'dot', 'remove', 'John')\n",
      "('dot', 'remove', 'John', 'would')\n",
      "('remove', 'John', 'would', 'able')\n",
      "('John', 'would', 'able', 'remove')\n",
      "('would', 'able', 'remove', 'John')\n",
      "('able', 'remove', 'John', 'list')\n",
      "('remove', 'John', 'list', 'order')\n",
      "('John', 'list', 'order', 'check')\n",
      "('list', 'order', 'check', 'let')\n",
      "('order', 'check', 'let', \"'s\")\n",
      "('check', 'let', \"'s\", 'hit')\n",
      "('let', \"'s\", 'hit', 'enter')\n",
      "(\"'s\", 'hit', 'enter', 'type')\n",
      "('hit', 'enter', 'type', 'Li')\n",
      "('enter', 'type', 'Li', 'hit')\n",
      "('type', 'Li', 'hit', 'enter')\n",
      "('Li', 'hit', 'enter', 'get')\n",
      "('hit', 'enter', 'get', 'list')\n",
      "('enter', 'get', 'list', 'John')\n",
      "('get', 'list', 'John', 'available')\n",
      "('list', 'John', 'available', 'hope')\n",
      "('John', 'available', 'hope', 'clear')\n",
      "('available', 'hope', 'clear', 'remove')\n",
      "('hope', 'clear', 'remove', 'item')\n",
      "('clear', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'remove')\n",
      "('item', 'using', 'remove', 'method')\n",
      "('using', 'remove', 'method', 'let')\n",
      "('remove', 'method', 'let', \"'s\")\n",
      "('method', 'let', \"'s\", 'move')\n",
      "('let', \"'s\", 'move', 'next')\n",
      "(\"'s\", 'move', 'next', 'topic')\n",
      "('move', 'next', 'topic', 'remove')\n",
      "('next', 'topic', 'remove', 'item')\n",
      "('topic', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'pop')\n",
      "('item', 'using', 'pop', 'method')\n",
      "('using', 'pop', 'method', 'pop')\n",
      "('pop', 'method', 'pop', 'method')\n",
      "('method', 'pop', 'method', 'used')\n",
      "('pop', 'method', 'used', 'remove')\n",
      "('method', 'used', 'remove', 'item')\n",
      "('used', 'remove', 'item', 'specified')\n",
      "('remove', 'item', 'specified', 'index')\n",
      "('item', 'specified', 'index', 'place')\n",
      "('specified', 'index', 'place', 'passing')\n",
      "('index', 'place', 'passing', 'item')\n",
      "('place', 'passing', 'item', 'pass')\n",
      "('passing', 'item', 'pass', 'index')\n",
      "('item', 'pass', 'index', 'item')\n",
      "('pass', 'index', 'item', 'pop')\n",
      "('index', 'item', 'pop', 'method')\n",
      "('item', 'pop', 'method', 'order')\n",
      "('pop', 'method', 'order', 'remove')\n",
      "('method', 'order', 'remove', 'item')\n",
      "('order', 'remove', 'item', 'example')\n",
      "('remove', 'item', 'example', 'consider')\n",
      "('item', 'example', 'consider', 'list')\n",
      "('example', 'consider', 'list', 'three')\n",
      "('consider', 'list', 'three', 'items')\n",
      "('list', 'three', 'items', 'John')\n",
      "('three', 'items', 'John', 'Mike')\n",
      "('items', 'John', 'Mike', 'David')\n",
      "('John', 'Mike', 'David', 'let')\n",
      "('Mike', 'David', 'let', \"'s\")\n",
      "('David', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'want')\n",
      "(\"'s\", 'say', 'want', 'remove')\n",
      "('say', 'want', 'remove', 'item')\n",
      "('want', 'remove', 'item', 'mic')\n",
      "('remove', 'item', 'mic', 'list')\n",
      "('item', 'mic', 'list', 'know')\n",
      "('mic', 'list', 'know', 'index')\n",
      "('list', 'know', 'index', 'item')\n",
      "('know', 'index', 'item', 'one')\n",
      "('index', 'item', 'one', 'need')\n",
      "('item', 'one', 'need', 'pass')\n",
      "('one', 'need', 'pass', 'index')\n",
      "('need', 'pass', 'index', 'item')\n",
      "('pass', 'index', 'item', 'pop')\n",
      "('index', 'item', 'pop', 'method')\n",
      "('item', 'pop', 'method', 'typing')\n",
      "('pop', 'method', 'typing', 'Li')\n",
      "('method', 'typing', 'Li', 'dot')\n",
      "('typing', 'Li', 'dot', 'pop')\n",
      "('Li', 'dot', 'pop', 'one')\n",
      "('dot', 'pop', 'one', 'remove')\n",
      "('pop', 'one', 'remove', 'item')\n",
      "('one', 'remove', 'item', 'list')\n",
      "('remove', 'item', 'list', 'one')\n",
      "('item', 'list', 'one', 'interesting')\n",
      "('list', 'one', 'interesting', 'thing')\n",
      "('one', 'interesting', 'thing', 'happen')\n",
      "('interesting', 'thing', 'happen', 'soon')\n",
      "('thing', 'happen', 'soon', 'hit')\n",
      "('happen', 'soon', 'hit', 'enter')\n",
      "('soon', 'hit', 'enter', 'get')\n",
      "('hit', 'enter', 'get', 'mic')\n",
      "('enter', 'get', 'mic', 'result')\n",
      "('get', 'mic', 'result', 'getting')\n",
      "('mic', 'result', 'getting', 'Mike')\n",
      "('result', 'getting', 'Mike', 'pop')\n",
      "('getting', 'Mike', 'pop', 'method')\n",
      "('Mike', 'pop', 'method', 'removes')\n",
      "('pop', 'method', 'removes', 'item')\n",
      "('method', 'removes', 'item', 'also')\n",
      "('removes', 'item', 'also', 'Returns')\n",
      "('item', 'also', 'Returns', 'deleted')\n",
      "('also', 'Returns', 'deleted', 'item')\n",
      "('Returns', 'deleted', 'item', 'use')\n",
      "('deleted', 'item', 'use', 'later')\n",
      "('item', 'use', 'later', 'code')\n",
      "('use', 'later', 'code', 'store')\n",
      "('later', 'code', 'store', 'item')\n",
      "('code', 'store', 'item', 'variable')\n",
      "('store', 'item', 'variable', 'use')\n",
      "('item', 'variable', 'use', 'item')\n",
      "('variable', 'use', 'item', 'code')\n",
      "('use', 'item', 'code', 'want')\n",
      "('item', 'code', 'want', 'pop')\n",
      "('code', 'want', 'pop', 'method')\n",
      "('want', 'pop', 'method', 'deletes')\n",
      "('pop', 'method', 'deletes', 'item')\n",
      "('method', 'deletes', 'item', 'also')\n",
      "('deletes', 'item', 'also', 'Returns')\n",
      "('item', 'also', 'Returns', 'deleted')\n",
      "('also', 'Returns', 'deleted', 'item')\n",
      "('Returns', 'deleted', 'item', 'whenever')\n",
      "('deleted', 'item', 'whenever', 'want')\n",
      "('item', 'whenever', 'want', 'use')\n",
      "('whenever', 'want', 'use', 'deleted')\n",
      "('want', 'use', 'deleted', 'item')\n",
      "('use', 'deleted', 'item', 'code')\n",
      "('deleted', 'item', 'code', 'use')\n",
      "('item', 'code', 'use', 'pop')\n",
      "('code', 'use', 'pop', 'method')\n",
      "('use', 'pop', 'method', 'otherwise')\n",
      "('pop', 'method', 'otherwise', 'use')\n",
      "('method', 'otherwise', 'use', 'remove')\n",
      "('otherwise', 'use', 'remove', 'method')\n",
      "('use', 'remove', 'method', 'difference')\n",
      "('remove', 'method', 'difference', 'remove')\n",
      "('method', 'difference', 'remove', 'method')\n",
      "('difference', 'remove', 'method', 'pop')\n",
      "('remove', 'method', 'pop', 'method')\n",
      "('method', 'pop', 'method', 'let')\n",
      "('pop', 'method', 'let', \"'s\")\n",
      "('method', 'let', \"'s\", 'check')\n",
      "('let', \"'s\", 'check', 'whether')\n",
      "(\"'s\", 'check', 'whether', 'item')\n",
      "('check', 'whether', 'item', 'deleted')\n",
      "('whether', 'item', 'deleted', 'list')\n",
      "('item', 'deleted', 'list', 'type')\n",
      "('deleted', 'list', 'type', 'Li')\n",
      "('list', 'type', 'Li', 'hit')\n",
      "('type', 'Li', 'hit', 'enter')\n",
      "('Li', 'hit', 'enter', 'get')\n",
      "('hit', 'enter', 'get', 'John')\n",
      "('enter', 'get', 'John', 'David')\n",
      "('get', 'John', 'David', 'list')\n",
      "('John', 'David', 'list', 'mic')\n",
      "('David', 'list', 'mic', 'list')\n",
      "('list', 'mic', 'list', 'one')\n",
      "('mic', 'list', 'one', 'interesting')\n",
      "('list', 'one', 'interesting', 'point')\n",
      "('one', 'interesting', 'point', 'would')\n",
      "('interesting', 'point', 'would', 'like')\n",
      "('point', 'would', 'like', 'mention')\n",
      "('would', 'like', 'mention', 'index')\n",
      "('like', 'mention', 'index', 'item')\n",
      "('mention', 'index', 'item', 'specified')\n",
      "('index', 'item', 'specified', 'last')\n",
      "('item', 'specified', 'last', 'item')\n",
      "('specified', 'last', 'item', 'deleted')\n",
      "('last', 'item', 'deleted', 'let')\n",
      "('item', 'deleted', 'let', \"'s\")\n",
      "('deleted', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'wo')\n",
      "(\"'s\", 'say', 'wo', \"n't\")\n",
      "('say', 'wo', \"n't\", 'pass')\n",
      "('wo', \"n't\", 'pass', 'index')\n",
      "(\"n't\", 'pass', 'index', 'pop')\n",
      "('pass', 'index', 'pop', 'method')\n",
      "('index', 'pop', 'method', 'case')\n",
      "('pop', 'method', 'case', 'pop')\n",
      "('method', 'case', 'pop', 'method')\n",
      "('case', 'pop', 'method', 'remove')\n",
      "('pop', 'method', 'remove', 'last')\n",
      "('method', 'remove', 'last', 'item')\n",
      "('remove', 'last', 'item', 'list')\n",
      "('last', 'item', 'list', 'example')\n",
      "('item', 'list', 'example', 'David')\n",
      "('list', 'example', 'David', 'deleted')\n",
      "('example', 'David', 'deleted', 'list')\n",
      "('David', 'deleted', 'list', 'pop')\n",
      "('deleted', 'list', 'pop', 'method')\n",
      "('list', 'pop', 'method', 'works')\n",
      "('pop', 'method', 'works', 'let')\n",
      "('method', 'works', 'let', \"'s\")\n",
      "('works', 'let', \"'s\", 'move')\n",
      "('let', \"'s\", 'move', 'next')\n",
      "(\"'s\", 'move', 'next', 'topic')\n",
      "('move', 'next', 'topic', 'remove')\n",
      "('next', 'topic', 'remove', 'item')\n",
      "('topic', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'Dell')\n",
      "('item', 'using', 'Dell', 'keyword')\n",
      "('using', 'Dell', 'keyword', 'Dell')\n",
      "('Dell', 'keyword', 'Dell', 'keyword')\n",
      "('keyword', 'Dell', 'keyword', 'also')\n",
      "('Dell', 'keyword', 'also', 'used')\n",
      "('keyword', 'also', 'used', 'remove')\n",
      "('also', 'used', 'remove', 'item')\n",
      "('used', 'remove', 'item', 'specified')\n",
      "('remove', 'item', 'specified', 'index')\n",
      "('item', 'specified', 'index', 'removes')\n",
      "('specified', 'index', 'removes', 'item')\n",
      "('index', 'removes', 'item', 'specified')\n",
      "('removes', 'item', 'specified', 'index')\n",
      "('item', 'specified', 'index', 'example')\n",
      "('specified', 'index', 'example', 'let')\n",
      "('index', 'example', 'let', \"'s\")\n",
      "('example', 'let', \"'s\", 'open')\n",
      "('let', \"'s\", 'open', 'Command')\n",
      "(\"'s\", 'open', 'Command', 'Prompt')\n",
      "('open', 'Command', 'Prompt', 'let')\n",
      "('Command', 'Prompt', 'let', \"'s\")\n",
      "('Prompt', 'let', \"'s\", 'consider')\n",
      "('let', \"'s\", 'consider', 'example')\n",
      "(\"'s\", 'consider', 'example', 'list')\n",
      "('consider', 'example', 'list', 'time')\n",
      "('example', 'list', 'time', 'interested')\n",
      "('list', 'time', 'interested', 'deleting')\n",
      "('time', 'interested', 'deleting', 'first')\n",
      "('interested', 'deleting', 'first', 'item')\n",
      "('deleting', 'first', 'item', 'list')\n",
      "('first', 'item', 'list', 'using')\n",
      "('item', 'list', 'using', 'Dell')\n",
      "('list', 'using', 'Dell', 'keyword')\n",
      "('using', 'Dell', 'keyword', 'need')\n",
      "('Dell', 'keyword', 'need', 'type')\n",
      "('keyword', 'need', 'type', 'Del')\n",
      "('need', 'type', 'Del', 'Li')\n",
      "('type', 'Del', 'Li', 'within')\n",
      "('Del', 'Li', 'within', 'brackets')\n",
      "('Li', 'within', 'brackets', '0.')\n",
      "('within', 'brackets', '0.', 'need')\n",
      "('brackets', '0.', 'need', 'specify')\n",
      "('0.', 'need', 'specify', 'index')\n",
      "('need', 'specify', 'index', 'item')\n",
      "('specify', 'index', 'item', 'want')\n",
      "('index', 'item', 'want', 'delete')\n",
      "('item', 'want', 'delete', 'first')\n",
      "('want', 'delete', 'first', 'thing')\n",
      "('delete', 'first', 'thing', 'need')\n",
      "('first', 'thing', 'need', 'type')\n",
      "('thing', 'need', 'type', 'Del')\n",
      "('need', 'type', 'Del', 'let')\n",
      "('type', 'Del', 'let', \"'s\")\n",
      "('Del', 'let', \"'s\", 'hit')\n",
      "('let', \"'s\", 'hit', 'enter')\n",
      "(\"'s\", 'hit', 'enter', 'type')\n",
      "('hit', 'enter', 'type', 'Li')\n",
      "('enter', 'type', 'Li', 'check')\n",
      "('type', 'Li', 'check', 'whether')\n",
      "('Li', 'check', 'whether', 'list')\n",
      "('check', 'whether', 'list', 'updated')\n",
      "('whether', 'list', 'updated', 'let')\n",
      "('list', 'updated', 'let', \"'s\")\n",
      "('updated', 'let', \"'s\", 'hit')\n",
      "('let', \"'s\", 'hit', 'enter')\n",
      "(\"'s\", 'hit', 'enter', 'get')\n",
      "('hit', 'enter', 'get', 'list')\n",
      "('enter', 'get', 'list', 'John')\n",
      "('get', 'list', 'John', 'available')\n",
      "('list', 'John', 'available', 'learned')\n",
      "('John', 'available', 'learned', 'remove')\n",
      "('available', 'learned', 'remove', 'item')\n",
      "('learned', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'Dell')\n",
      "('item', 'using', 'Dell', 'keyboard')\n",
      "('using', 'Dell', 'keyboard', 'Dell')\n",
      "('Dell', 'keyboard', 'Dell', 'keyword')\n",
      "('keyboard', 'Dell', 'keyword', 'also')\n",
      "('Dell', 'keyword', 'also', 'useful')\n",
      "('keyword', 'also', 'useful', 'removing')\n",
      "('also', 'useful', 'removing', 'complete')\n",
      "('useful', 'removing', 'complete', 'list')\n",
      "('removing', 'complete', 'list', 'remove')\n",
      "('complete', 'list', 'remove', 'entire')\n",
      "('list', 'remove', 'entire', 'list')\n",
      "('remove', 'entire', 'list', 'also')\n",
      "('entire', 'list', 'also', 'Dell')\n",
      "('list', 'also', 'Dell', 'keyword')\n",
      "('also', 'Dell', 'keyword', 'let')\n",
      "('Dell', 'keyword', 'let', \"'s\")\n",
      "('keyword', 'let', \"'s\", 'open')\n",
      "('let', \"'s\", 'open', 'Command')\n",
      "(\"'s\", 'open', 'Command', 'Prompt')\n",
      "('open', 'Command', 'Prompt', 'let')\n",
      "('Command', 'Prompt', 'let', \"'s\")\n",
      "('Prompt', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'want')\n",
      "(\"'s\", 'say', 'want', 'delete')\n",
      "('say', 'want', 'delete', 'entire')\n",
      "('want', 'delete', 'entire', 'list')\n",
      "('delete', 'entire', 'list', 'type')\n",
      "('entire', 'list', 'type', 'Del')\n",
      "('list', 'type', 'Del', 'Li')\n",
      "('type', 'Del', 'Li', \"'s\")\n",
      "('Del', 'Li', \"'s\", 'specify')\n",
      "('Li', \"'s\", 'specify', 'name')\n",
      "(\"'s\", 'specify', 'name', 'list')\n",
      "('specify', 'name', 'list', 'hitting')\n",
      "('name', 'list', 'hitting', 'enter')\n",
      "('list', 'hitting', 'enter', 'list')\n",
      "('hitting', 'enter', 'list', 'deleted')\n",
      "('enter', 'list', 'deleted', 'let')\n",
      "('list', 'deleted', 'let', \"'s\")\n",
      "('deleted', 'let', \"'s\", 'type')\n",
      "('let', \"'s\", 'type', 'Li')\n",
      "(\"'s\", 'type', 'Li', 'check')\n",
      "('type', 'Li', 'check', 'whether')\n",
      "('Li', 'check', 'whether', 'list')\n",
      "('check', 'whether', 'list', 'deleted')\n",
      "('whether', 'list', 'deleted', 'soon')\n",
      "('list', 'deleted', 'soon', 'hit')\n",
      "('deleted', 'soon', 'hit', 'enter')\n",
      "('soon', 'hit', 'enter', 'get')\n",
      "('hit', 'enter', 'get', 'name')\n",
      "('enter', 'get', 'name', 'error')\n",
      "('get', 'name', 'error', 'Interpreter')\n",
      "('name', 'error', 'Interpreter', 'says')\n",
      "('error', 'Interpreter', 'says', 'name')\n",
      "('Interpreter', 'says', 'name', 'Li')\n",
      "('says', 'name', 'Li', 'defined')\n",
      "('name', 'Li', 'defined', 'clearly')\n",
      "('Li', 'defined', 'clearly', 'indicates')\n",
      "('defined', 'clearly', 'indicates', 'Li')\n",
      "('clearly', 'indicates', 'Li', 'exist')\n",
      "('indicates', 'Li', 'exist', 'means')\n",
      "('Li', 'exist', 'means', 'list')\n",
      "('exist', 'means', 'list', 'completely')\n",
      "('means', 'list', 'completely', 'deleted')\n",
      "('list', 'completely', 'deleted', 'Dell')\n",
      "('completely', 'deleted', 'Dell', 'keyword')\n",
      "('deleted', 'Dell', 'keyword', 'useful')\n",
      "('Dell', 'keyword', 'useful', 'want')\n",
      "('keyword', 'useful', 'want', 'delete')\n",
      "('useful', 'want', 'delete', 'entire')\n",
      "('want', 'delete', 'entire', 'list')\n",
      "('delete', 'entire', 'list', 'let')\n",
      "('entire', 'list', 'let', \"'s\")\n",
      "('list', 'let', \"'s\", 'move')\n",
      "('let', \"'s\", 'move', 'next')\n",
      "(\"'s\", 'move', 'next', 'topic')\n",
      "('move', 'next', 'topic', 'clear')\n",
      "('next', 'topic', 'clear', 'list')\n",
      "('topic', 'clear', 'list', 'using')\n",
      "('clear', 'list', 'using', 'clear')\n",
      "('list', 'using', 'clear', 'method')\n",
      "('using', 'clear', 'method', 'let')\n",
      "('clear', 'method', 'let', \"'s\")\n",
      "('method', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'requirement')\n",
      "(\"'s\", 'say', 'requirement', 'delete')\n",
      "('say', 'requirement', 'delete', 'entire')\n",
      "('requirement', 'delete', 'entire', 'list')\n",
      "('delete', 'entire', 'list', 'clear')\n",
      "('entire', 'list', 'clear', 'list')\n",
      "('list', 'clear', 'list', 'empty')\n",
      "('clear', 'list', 'empty', 'list')\n",
      "('list', 'empty', 'list', 'use')\n",
      "('empty', 'list', 'use', 'clear')\n",
      "('list', 'use', 'clear', 'method')\n",
      "('use', 'clear', 'method', 'purpose')\n",
      "('clear', 'method', 'purpose', 'clear')\n",
      "('method', 'purpose', 'clear', 'method')\n",
      "('purpose', 'clear', 'method', 'empty')\n",
      "('clear', 'method', 'empty', 'list')\n",
      "('method', 'empty', 'list', 'example')\n",
      "('empty', 'list', 'example', 'let')\n",
      "('list', 'example', 'let', \"'s\")\n",
      "('example', 'let', \"'s\", 'open')\n",
      "('let', \"'s\", 'open', 'Command')\n",
      "(\"'s\", 'open', 'Command', 'Prompt')\n",
      "('open', 'Command', 'Prompt', 'consider')\n",
      "('Command', 'Prompt', 'consider', 'list')\n",
      "('Prompt', 'consider', 'list', 'let')\n",
      "('consider', 'list', 'let', \"'s\")\n",
      "('list', 'let', \"'s\", 'say')\n",
      "('let', \"'s\", 'say', 'want')\n",
      "(\"'s\", 'say', 'want', 'empty')\n",
      "('say', 'want', 'empty', 'list')\n",
      "('want', 'empty', 'list', 'type')\n",
      "('empty', 'list', 'type', 'Li')\n",
      "('list', 'type', 'Li', 'dot')\n",
      "('type', 'Li', 'dot', 'clear')\n",
      "('Li', 'dot', 'clear', 'let')\n",
      "('dot', 'clear', 'let', \"'s\")\n",
      "('clear', 'let', \"'s\", 'hit')\n",
      "('let', \"'s\", 'hit', 'enter')\n",
      "(\"'s\", 'hit', 'enter', 'type')\n",
      "('hit', 'enter', 'type', 'Li')\n",
      "('enter', 'type', 'Li', 'let')\n",
      "('type', 'Li', 'let', \"'s\")\n",
      "('Li', 'let', \"'s\", 'hit')\n",
      "('let', \"'s\", 'hit', 'enter')\n",
      "(\"'s\", 'hit', 'enter', 'get')\n",
      "('hit', 'enter', 'get', 'empty')\n",
      "('enter', 'get', 'empty', 'list')\n",
      "('get', 'empty', 'list', 'get')\n",
      "('empty', 'list', 'get', 'error')\n",
      "('list', 'get', 'error', 'time')\n",
      "('get', 'error', 'time', 'list')\n",
      "('error', 'time', 'list', 'completely')\n",
      "('time', 'list', 'completely', 'deleted')\n",
      "('list', 'completely', 'deleted', 'still')\n",
      "('completely', 'deleted', 'still', 'list')\n",
      "('deleted', 'still', 'list', 'time')\n",
      "('still', 'list', 'time', 'list')\n",
      "('list', 'time', 'list', 'empty')\n",
      "('time', 'list', 'empty', 'clear')\n",
      "('list', 'empty', 'clear', 'method')\n",
      "('empty', 'clear', 'method', 'used')\n",
      "('clear', 'method', 'used', 'remove')\n",
      "('method', 'used', 'remove', 'items')\n",
      "('used', 'remove', 'items', 'list')\n",
      "('remove', 'items', 'list', 'remove')\n",
      "('items', 'list', 'remove', 'entire')\n",
      "('list', 'remove', 'entire', 'list')\n",
      "('remove', 'entire', 'list', 'difference')\n",
      "('entire', 'list', 'difference', 'Dell')\n",
      "('list', 'difference', 'Dell', 'clear')\n",
      "('difference', 'Dell', 'clear', 'hope')\n",
      "('Dell', 'clear', 'hope', 'concept')\n",
      "('clear', 'hope', 'concept', 'clear')\n",
      "('hope', 'concept', 'clear', 'method')\n",
      "('concept', 'clear', 'method', 'also')\n",
      "('clear', 'method', 'also', 'clear')\n",
      "('method', 'also', 'clear', 'done')\n",
      "('also', 'clear', 'done', 'topics')\n",
      "('clear', 'done', 'topics', 'presentation')\n",
      "('done', 'topics', 'presentation', 'learned')\n",
      "('topics', 'presentation', 'learned', 'remove')\n",
      "('presentation', 'learned', 'remove', 'item')\n",
      "('learned', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'remove')\n",
      "('item', 'using', 'remove', 'method')\n",
      "('using', 'remove', 'method', 'learned')\n",
      "('remove', 'method', 'learned', 'remove')\n",
      "('method', 'learned', 'remove', 'item')\n",
      "('learned', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'pop')\n",
      "('item', 'using', 'pop', 'method')\n",
      "('using', 'pop', 'method', 'seen')\n",
      "('pop', 'method', 'seen', 'difference')\n",
      "('method', 'seen', 'difference', 'two')\n",
      "('seen', 'difference', 'two', 'difference')\n",
      "('difference', 'two', 'difference', 'lies')\n",
      "('two', 'difference', 'lies', 'fact')\n",
      "('difference', 'lies', 'fact', 'pop')\n",
      "('lies', 'fact', 'pop', 'method')\n",
      "('fact', 'pop', 'method', 'deletes')\n",
      "('pop', 'method', 'deletes', 'item')\n",
      "('method', 'deletes', 'item', 'also')\n",
      "('deletes', 'item', 'also', 'Returns')\n",
      "('item', 'also', 'Returns', 'deleted')\n",
      "('also', 'Returns', 'deleted', 'item')\n",
      "('Returns', 'deleted', 'item', 'hand')\n",
      "('deleted', 'item', 'hand', 'remove')\n",
      "('item', 'hand', 'remove', 'method')\n",
      "('hand', 'remove', 'method', 'removes')\n",
      "('remove', 'method', 'removes', 'item')\n",
      "('method', 'removes', 'item', 'list')\n",
      "('removes', 'item', 'list', 'learned')\n",
      "('item', 'list', 'learned', 'remove')\n",
      "('list', 'learned', 'remove', 'item')\n",
      "('learned', 'remove', 'item', 'using')\n",
      "('remove', 'item', 'using', 'Dell')\n",
      "('item', 'using', 'Dell', 'keyword')\n",
      "('using', 'Dell', 'keyword', 'also')\n",
      "('Dell', 'keyword', 'also', 'want')\n",
      "('keyword', 'also', 'want', 'delete')\n",
      "('also', 'want', 'delete', 'entire')\n",
      "('want', 'delete', 'entire', 'list')\n",
      "('delete', 'entire', 'list', 'Dell')\n",
      "('entire', 'list', 'Dell', 'keyword')\n",
      "('list', 'Dell', 'keyword', 'useful')\n",
      "('Dell', 'keyword', 'useful', 'also')\n",
      "('keyword', 'useful', 'also', 'learned')\n",
      "('useful', 'also', 'learned', 'clear')\n",
      "('also', 'learned', 'clear', 'list')\n",
      "('learned', 'clear', 'list', 'empty')\n",
      "('clear', 'list', 'empty', 'list')\n",
      "('list', 'empty', 'list', 'using')\n",
      "('empty', 'list', 'using', 'clear')\n",
      "('list', 'using', 'clear', 'method')\n",
      "('using', 'clear', 'method', 'hope')\n",
      "('clear', 'method', 'hope', 'concepts')\n",
      "('method', 'hope', 'concepts', 'presentation')\n",
      "('hope', 'concepts', 'presentation', 'completely')\n",
      "('concepts', 'presentation', 'completely', 'clear')\n",
      "('presentation', 'completely', 'clear', 'okay')\n",
      "('completely', 'clear', 'okay', 'friends')\n",
      "('clear', 'okay', 'friends', 'thank')\n",
      "('okay', 'friends', 'thank', 'watching')\n",
      "('friends', 'thank', 'watching', 'presentation')\n",
      "('thank', 'watching', 'presentation', 'see')\n",
      "('watching', 'presentation', 'see', 'next')\n",
      "('presentation', 'see', 'next', 'one')\n",
      "('see', 'next', 'one', '[')\n",
      "('next', 'one', '[', 'Music')\n",
      "('one', '[', 'Music', ']')\n",
      "('[', 'Music', ']', '[')\n",
      "('Music', ']', '[', 'Applause')\n",
      "(']', '[', 'Applause', ']')\n",
      "('[', 'Applause', ']', '[')\n",
      "('Applause', ']', '[', 'Music')\n",
      "(']', '[', 'Music', ']')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sanatan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Define the n-gram size\n",
    "n =4\n",
    "\n",
    "# Generate the n-grams\n",
    "ngrams = list(nltk.ngrams(words, n))\n",
    "\n",
    "# Print the n-grams\n",
    "for gram in ngrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes:\n",
      "- and\n",
      "- text\n",
      "- it\n",
      "- This\n",
      "- is\n",
      "- an\n",
      "- example\n",
      "- text\n",
      "- It\n",
      "- contains\n",
      "- useful\n",
      "- information\n",
      "- that\n",
      "- we\n",
      "- want\n",
      "- to\n",
      "- generate\n",
      "- notes\n",
      "- and\n",
      "- a\n",
      "- summary\n",
      "- from\n",
      "- The\n",
      "- text\n",
      "- includes\n",
      "- repetitive\n",
      "- patterns\n",
      "- and\n",
      "- important\n",
      "- phrases\n",
      "- It\n",
      "- also\n",
      "- has\n",
      "- formulas\n",
      "- like\n",
      "- E\n",
      "- mc\n",
      "- and\n",
      "- F\n",
      "- ma\n",
      "- We\n",
      "- will\n",
      "- use\n",
      "- techniques\n",
      "- like\n",
      "- n\n",
      "- grams\n",
      "- sequence\n",
      "- analysis\n",
      "- and\n",
      "- TF\n",
      "- IDF\n",
      "- to\n",
      "- extract\n",
      "- the\n",
      "- relevant\n",
      "- information\n",
      "\n",
      "Summary:\n",
      "- \n",
      "This is an example text\n",
      "- It contains useful information that we want to generate notes and a summary from\n",
      "- \n",
      "The text includes repetitive patterns and important phrases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Preprocess the text by removing punctuation and converting to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text\n",
    "\n",
    "def extract_phrases(text, n):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Generate n-grams\n",
    "    ngram_list = list(ngrams(tokens, n))\n",
    "    \n",
    "    # Convert n-grams back to phrases\n",
    "    phrases = [' '.join(gram) for gram in ngram_list]\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "def calculate_tfidf(texts):\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def extract_formulas(text):\n",
    "    # Use regular expressions to detect formulas\n",
    "    formula_pattern = r'\\b[a-zA-Z]+\\b'  # Modify the pattern as per your formula detection requirements\n",
    "    formulas = re.findall(formula_pattern, text)\n",
    "    \n",
    "    return formulas\n",
    "\n",
    "def generate_notes_and_summary(text, n, top_n):\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Extract phrases using n-grams\n",
    "    phrases = extract_phrases(preprocessed_text, n)\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    vectorizer, tfidf_matrix = calculate_tfidf(phrases)\n",
    "    \n",
    "    # Get the feature names (phrases)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Identify important phrases based on TF-IDF scores\n",
    "    tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "    important_indices = tfidf_scores.argsort()[-top_n:][::-1]\n",
    "    important_phrases = [feature_names[i] for i in important_indices]\n",
    "    \n",
    "    # Extract formulas\n",
    "    formulas = extract_formulas(text)\n",
    "    \n",
    "    # Generate notes by combining important phrases and formulas\n",
    "    notes = important_phrases + formulas\n",
    "    \n",
    "    # Generate summary using extractive summarization (top sentences based on cosine similarity)\n",
    "    sentences = text.split('. ')  # Split text into sentences\n",
    "    sentence_vectors = vectorizer.transform(sentences)\n",
    "    similarity_scores = cosine_similarity(sentence_vectors, tfidf_matrix)\n",
    "    top_sentences_indices = np.argsort(similarity_scores.flatten())[-top_n:][::-1]\n",
    "    summary = [sentences[i] for i in top_sentences_indices]\n",
    "    \n",
    "    return notes, summary\n",
    "\n",
    "# Example usage\n",
    "input_text = \"\"\"\n",
    "This is an example text. It contains useful information that we want to generate notes and a summary from. \n",
    "The text includes repetitive patterns and important phrases. It also has formulas like E=mc^2 and F=ma. \n",
    "We will use techniques like n-grams, sequence analysis, and TF-IDF to extract the relevant information.\n",
    "\"\"\"\n",
    "\n",
    "n_gram_length = 2\n",
    "top_n_phrases = 3\n",
    "\n",
    "notes, summary = generate_notes_and_summary(input_text, n_gram_length, top_n_phrases)\n",
    "\n",
    "print(\"Notes:\")\n",
    "for note in notes:\n",
    "    print(\"- \" + note)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for sentence in summary:\n",
    "    print(\"- \" + sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SpeechRecognition in /home/sanatan/.local/lib/python3.8/site-packages (3.10.0)\n",
      "Requirement already satisfied: moviepy in /home/sanatan/.local/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/sanatan/.local/lib/python3.8/site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/sanatan/.local/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/sanatan/.local/lib/python3.8/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/sanatan/.local/lib/python3.8/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/sanatan/.local/lib/python3.8/site-packages (from moviepy) (1.24.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/sanatan/.local/lib/python3.8/site-packages (from moviepy) (2.31.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/sanatan/.local/lib/python3.8/site-packages (from moviepy) (0.4.8)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/sanatan/.local/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sanatan/.local/lib/python3.8/site-packages (from requests>=2.26.0->SpeechRecognition) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2019.11.28)\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 17:34:39.905353: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 17:34:46.207887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-07 17:34:46.210665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-07 17:34:46.210860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-08-07 17:34:53.017346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-08-07 17:34:53.018426: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-07 17:34:53.018511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MSI): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m semantic_network \u001b[39m=\u001b[39m build_semantic_network(noun_phrases)\n\u001b[1;32m     42\u001b[0m \u001b[39m# Add relationships\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m add_relationship(semantic_network, \u001b[39m'\u001b[39;49m\u001b[39mcat\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mis_a\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmammal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m add_relationship(semantic_network, \u001b[39m'\u001b[39m\u001b[39mfur\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpart_of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m add_relationship(semantic_network, \u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhas\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msharp claws\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36madd_relationship\u001b[0;34m(semantic_network, source, relationship, target)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_relationship\u001b[39m(semantic_network, source, relationship, target):\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mif\u001b[39;00m relationship \u001b[39min\u001b[39;00m semantic_network[source]:\n\u001b[1;32m     28\u001b[0m         semantic_network[source][relationship]\u001b[39m.\u001b[39mappend(target)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cat'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_noun_phrases(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    noun_phrases = []\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_phrases.append(chunk.text)\n",
    "    \n",
    "    return noun_phrases\n",
    "\n",
    "def build_semantic_network(noun_phrases):\n",
    "    semantic_network = {}\n",
    "    \n",
    "    for np in noun_phrases:\n",
    "        semantic_network[np] = {\n",
    "            'is_a': [],\n",
    "            'part_of': [],\n",
    "            'has': []\n",
    "        }\n",
    "    \n",
    "    return semantic_network\n",
    "\n",
    "def add_relationship(semantic_network, source, relationship, target):\n",
    "    if relationship in semantic_network[source]:\n",
    "        semantic_network[source][relationship].append(target)\n",
    "    \n",
    "    # If bidirectional relationship is desired, uncomment the following lines\n",
    "    # if relationship in semantic_network[target]:\n",
    "    #     semantic_network[target][relationship].append(source)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"\"\"\n",
    "The cat is a mammal. The cat has fur. The fur is part of the cat. The cat has sharp claws. Claws are sharp.\n",
    "\"\"\"\n",
    "\n",
    "noun_phrases = extract_noun_phrases(input_text)\n",
    "semantic_network = build_semantic_network(noun_phrases)\n",
    "\n",
    "# Add relationships\n",
    "add_relationship(semantic_network, 'cat', 'is_a', 'mammal')\n",
    "add_relationship(semantic_network, 'fur', 'part_of', 'cat')\n",
    "add_relationship(semantic_network, 'cat', 'has', 'sharp claws')\n",
    "add_relationship(semantic_network, 'claws', 'are', 'sharp')\n",
    "\n",
    "# Print the semantic network\n",
    "for node, relationships in semantic_network.items():\n",
    "    print(f\"{node}:\")\n",
    "    for relationship, targets in relationships.items():\n",
    "        print(f\"  {relationship}: {', '.join(targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "video_path = \"../videos/neso.mp4\"\n",
    "video = VideoFileClip(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_video_segment(segment):\n",
    "    audio = segment.audio\n",
    "    audio_file = \"temp_audio.wav\"\n",
    "    audio.write_audiofile(audio_file)\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        transcription = recognizer.recognize_sphinx(audio_data)\n",
    "        \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 0/9070 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "recognition request failed: Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/speech_recognition/__init__.py:708\u001b[0m, in \u001b[0;36mRecognizer.recognize_google\u001b[0;34m(self, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     response \u001b[39m=\u001b[39m urlopen(request, timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moperation_timeout)\n\u001b[1;32m    709\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    641\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    643\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transcription \u001b[39m=\u001b[39m transcribe_video(video_path)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(transcription)\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mtranscribe_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m sr\u001b[39m.\u001b[39mAudioFile(audio_file) \u001b[39mas\u001b[39;00m source:\n\u001b[1;32m      7\u001b[0m     audio_data \u001b[39m=\u001b[39m recognizer\u001b[39m.\u001b[39mrecord(source)\n\u001b[0;32m----> 8\u001b[0m     transcription \u001b[39m=\u001b[39m recognizer\u001b[39m.\u001b[39;49mrecognize_google(audio_data)\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m transcription\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/speech_recognition/__init__.py:710\u001b[0m, in \u001b[0;36mRecognizer.recognize_google\u001b[0;34m(self, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[1;32m    708\u001b[0m     response \u001b[39m=\u001b[39m urlopen(request, timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation_timeout)\n\u001b[1;32m    709\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m RequestError(\u001b[39m\"\u001b[39m\u001b[39mrecognition request failed: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e\u001b[39m.\u001b[39mreason))\n\u001b[1;32m    711\u001b[0m \u001b[39mexcept\u001b[39;00m URLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    712\u001b[0m     \u001b[39mraise\u001b[39;00m RequestError(\u001b[39m\"\u001b[39m\u001b[39mrecognition connection failed: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e\u001b[39m.\u001b[39mreason))\n",
      "\u001b[0;31mRequestError\u001b[0m: recognition request failed: Bad Request"
     ]
    }
   ],
   "source": [
    "transcription = transcribe_video(video_path)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Convert your training, validation, and test sets to sequences of word indices\u001b[39;00m\n\u001b[1;32m     33\u001b[0m train_sequences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(text)\n\u001b[0;32m---> 34\u001b[0m val_sequences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(val_texts)\n\u001b[1;32m     35\u001b[0m test_sequences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(test_texts)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Pad the sequences to have uniform length\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_texts' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum length of your input sequences\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Define the number of LSTM units\n",
    "LSTM_UNITS = 128\n",
    "\n",
    "# Define the number of output classes\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Define the vocabulary size\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Define the embedding dimension\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Define the number of training epochs\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Prepare your data\n",
    "text = \"Remember to preprocess your summarized text into word indices or word embeddings before feeding it into the LSTM model. Also, consider any additional preprocessing steps, such as handling out-of-vocabulary words or performing data augmentation, based on your specific needs.\"\n",
    "\n",
    "# Instantiate the tokenizer object\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "# Convert your training, validation, and test sets to sequences of word indices\n",
    "train_sequences = tokenizer.texts_to_sequences(text)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad the sequences to have uniform length\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_data = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Convert your target labels to one-hot encoding\n",
    "train_labels = np.eye(NUM_CLASSES)[train_labels]\n",
    "val_labels = np.eye(NUM_CLASSES)[val_labels]\n",
    "test_labels = np.eye(NUM_CLASSES)[test_labels]\n",
    "\n",
    "# Build and train the LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(LSTM_UNITS))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=NUM_EPOCHS, batch_size=64)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data, test_labels, batch_size=64)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.5 MB 114 kB/s eta 0:00:01    |██████████                      | 8.2 MB 7.4 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/sanatan/.local/lib/python3.8/site-packages (from gensim) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/sanatan/.local/lib/python3.8/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/sanatan/.local/lib/python3.8/site-packages (from gensim) (6.3.0)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[') /']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "pattern = r'\\b[-+*/()^\\d\\s]+\\b|\\b[-+*/()^\\d\\s]+(?=\\s*=\\s*[-+*/()^\\d\\s]+\\b)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "matches = re.findall(pattern, \"In physics, various formulas play a fundamental role in describing the laws of nature. Let's start with Newton's second law of motion, which states that the force acting on an object is equal to its mass multiplied by its acceleration: F = ma. We also encounter the formula for calculating the gravitational force between two objects. According to Newton's law of universal gravitation, the gravitational force (F) between two objects is given by the equation F = (G * m1 * m2) / r^2, where G is the gravitational constant, m1 and m2 are the masses of the objects, and r is the distance between their centers.\")\n",
    "\n",
    "    # Filter out matches that are likely to be simple arithmetic expressions\n",
    "formulas = [match.strip() for match in matches if len(match.split()) > 1]\n",
    "\n",
    "formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b * d= c', 'bx + c = 0', 'a = b', 'c = d', 'a + b = 5', 'Z=F*M']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"The equation a + b * d= c represents addition. The quadratic equation ax^2 + bx + c = 0 has solutions. The logical equation a = b AND c = d is true. Another example is a + b = 5. Z=F*M\"\n",
    "\n",
    "equations = re.findall(r'\\b\\w+(?:\\s*\\^\\s*\\w+)?\\s*(?:[-+*/]\\s*\\w+(?:\\s*\\^\\s*\\w+)?)*\\s*=\\s*(?:\\w+|\\d+)(?:\\s*[-+*/]\\s*(?:\\w+|\\d+)(?:\\s*\\^\\s*\\w+)?)*\\b', text)\n",
    "print(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F = G* (m1m2/d2^2)', '{(2x+3)* (23x+3)} = 256', 'F = M*a']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text ='Evaluate the following equation Y=β₀+β₁*X+ε, where m is slope and c is intercept. Rearrange the following: overflow stack. Find x if F = G* (m1m2/d2^2). Find x if {(2x+3)* (23x+3)} = 256. Find x if F = M*a.'\n",
    "\n",
    "math_patterns = (r'((y|x|α|γ|β|ε)\\s*=\\s*.+),', r'if\\s(.+)', r'({.+)')\n",
    "sentences = text.split('.')\n",
    "\n",
    "formulas = []\n",
    "for x in sentences:\n",
    "    for pattern in math_patterns:\n",
    "        if re.search(pattern, x):\n",
    "            formulas.append(re.search(pattern, x).group(1))\n",
    "            break\n",
    "\n",
    "formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bsemantic_net = nx.Graph()\n",
    "\n",
    "# Add concepts as nodes\n",
    "semantic_net.add_node(\"Animal\")\n",
    "semantic_net.add_node(\"Mammal\")\n",
    "semantic_net.add_node(\"Dog\")\n",
    "semantic_net.add_node(\"Cat\")\n",
    "\n",
    "# Add relationships as edges\n",
    "semantic_net.add_edge(\"Animal\", \"Mammal\", relationship=\"is a\")\n",
    "semantic_net.add_edge(\"Mammal\", \"Dog\", relationship=\"is a\")\n",
    "semantic_net.add_edge(\"Mammal\", \"Cat\", relationship=\"is a\")\n",
    "\n",
    "# Visualize the semantic net\n",
    "pos = nx.spring_layout(semantic_net)\n",
    "nx.draw_networkx(semantic_net, pos, with_labels=True, node_color=\"lightblue\", font_size=10, node_size=1000)\n",
    "\n",
    "# Add relationship labels\n",
    "edge_labels = nx.get_edge_attributes(semantic_net, \"relationship\")\n",
    "nx.draw_networkx_edge_labels(semantic_net, pos, edge_labels=edge_labels)\n",
    "\n",
    "# Display the semantic net\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sanatan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sanatan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP'), ('with', 'IN'), ('NLTK', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary resources (only required for the first run)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Text to be analyzed\n",
    "text = \"I am learning NLP with NLTK.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Process each sentence and find POS tags\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Find POS tags for the words\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    # Print the POS tags\n",
    "    print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordcorenlp\n",
      "  Downloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: requests in /home/sanatan/.local/lib/python3.8/site-packages (from stanfordcorenlp) (2.31.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from stanfordcorenlp) (5.5.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->stanfordcorenlp) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sanatan/.local/lib/python3.8/site-packages (from requests->stanfordcorenlp) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->stanfordcorenlp) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->stanfordcorenlp) (2.8)\n",
      "Installing collected packages: stanfordcorenlp\n",
      "Successfully installed stanfordcorenlp-3.9.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence?\n",
      "another sentence follows?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_questions(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Set up the list of stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords and punctuation\n",
    "        words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "        # Generate questions\n",
    "        if words:\n",
    "            question = \" \".join(words) + \"?\"\n",
    "            questions.append(question)\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence. Another sentence follows.\"\n",
    "questions = generate_questions(text)\n",
    "for question in questions:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m squad_json_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../dataset/dev-v2.0.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     26\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../dataset/dev.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m squad_json_to_csv(squad_json_file, csv_file)\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36msquad_json_to_csv\u001b[0;34m(json_file, csv_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m qa \u001b[39min\u001b[39;00m paragraph[\u001b[39m'\u001b[39m\u001b[39mqas\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     17\u001b[0m     question \u001b[39m=\u001b[39m qa[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     answer_text \u001b[39m=\u001b[39m qa[\u001b[39m'\u001b[39;49m\u001b[39manswers\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     answer_start \u001b[39m=\u001b[39m qa[\u001b[39m'\u001b[39m\u001b[39manswers\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m     writer\u001b[39m.\u001b[39mwriterow([context, question, answer_text, answer_start])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def squad_json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(csv_file, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['context', 'question', 'answer_text', 'answer_start'])\n",
    "\n",
    "        for article in data['data']:\n",
    "            for paragraph in article['paragraphs']:\n",
    "                context = paragraph['context']\n",
    "\n",
    "                for qa in paragraph['qas']:\n",
    "                    question = qa['question']\n",
    "                    answer_text = qa['answers'][0]['text']\n",
    "                    answer_start = qa['answers'][0]['answer_start']\n",
    "                    writer.writerow([context, question, answer_text, answer_start])\n",
    "\n",
    "    print(f\"Conversion completed. CSV file saved as {csv_file}.\")\n",
    "\n",
    "# Example usage\n",
    "squad_json_file = '../dataset/dev-v2.0.json'\n",
    "csv_file = '../dataset/dev.csv'\n",
    "\n",
    "squad_json_to_csv(squad_json_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: A reconciliation of these views is suggested by William Atwood: \"Undoubtedly [Chopin's] use of traditional musical forms like the polonaise and mazurka roused nationalistic sentiments and a sense of cohesiveness amongst those Poles scattered across Europe and the New World ... While some sought solace in [them], others found them a source of strength in their continuing struggle for freedom. Although Chopin's music undoubtedly came to him intuitively rather than through any conscious patriotic design, it served all the same to symbolize the will of the Polish people ...\"\n",
      "Questions: [\"William Atwood suggested that Chopin's music wasn't purposely patriotic but what?\"\n",
      " \"A modern commentator, William Atwood, feels Poles not only sought solace in Chopin's music but also found them a source of strength as they continued to fight for what?\"\n",
      " 'Where were Poles scattered to?']\n",
      "Answers: ['intuitive' 'freedom' 'Europe and the New World']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../dataset/squad.csv')\n",
    "grouped = data.groupby(data['context'])\n",
    "for context, group in grouped:\n",
    "    print(\"Context:\", context)\n",
    "    print(\"Questions:\", group['question'].values)\n",
    "    print(\"Answers:\", group['answer_text'].values)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Sanatan', 'boy', {'relationship': 'is a'}], ['is', '.', {'relationship': 'is a'}])\n",
      "('Sanatan', 'boy', 'is', '.')\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     nx\u001b[39m.\u001b[39mdraw_networkx_edge_labels(semantic_net, pos, edge_labels\u001b[39m=\u001b[39medge_labels)\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m plt\n\u001b[0;32m---> 38\u001b[0m is_a_relationship(\u001b[39m'\u001b[39;49m\u001b[39mSanatan is a boy.\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     40\u001b[0m is_a_relationship(\u001b[39m'\u001b[39m\u001b[39mSanatan is a boy.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m, in \u001b[0;36mis_a_relationship\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     29\u001b[0m     semantic_net\u001b[39m.\u001b[39madd_node(nodes)\n\u001b[0;32m---> 30\u001b[0m     semantic_net\u001b[39m.\u001b[39madd_edge(edges[x][\u001b[39m0\u001b[39m], edges[x][\u001b[39m1\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39medges[x][\u001b[39m2\u001b[39m]) \n\u001b[1;32m     31\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m     32\u001b[0m pos \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mspring_layout(semantic_net)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "import re\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def n_grams(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    n =4\n",
    "    ngrams = list(nltk.ngrams(words, n))\n",
    "    return(ngrams)\n",
    "\n",
    "def is_a_relationship(text):\n",
    "    semantic_net = nx.Graph()\n",
    "    grams = n_grams(text)\n",
    "    list_ = ['is', 'is a', 'type', 'kind', 'category', 'class', 'group', 'species', 'example', 'instance', 'form', 'variety', 'model', 'variation', 'subtype', 'subcategory', 'subclass', 'subgroup', 'subspecies', 'subexample', 'subinstance', 'subform']\n",
    "    nodes = ()\n",
    "    edges = ()\n",
    "    for x in grams:\n",
    "        for j in x:\n",
    "            if j in list_:\n",
    "                nodes = nodes + (x[0], x[3], )\n",
    "                edges = edges + ([x[0], x[3], {'relationship': 'is a'}], )\n",
    "    print(edges)\n",
    "    print(nodes)\n",
    "    for x in range(len(nodes)):\n",
    "        print(x)\n",
    "        semantic_net.add_node(nodes)\n",
    "        semantic_net.add_edge(edges[x][0], edges[x][1], **edges[x][2]) \n",
    "    plt.figure()\n",
    "    pos = nx.spring_layout(semantic_net)\n",
    "    nx.draw_networkx(semantic_net, pos, with_labels=True, node_color=\"lightblue\", font_size=10, node_size=1000)\n",
    "    edge_labels = nx.get_edge_attributes(semantic_net, \"relationship\")\n",
    "    nx.draw_networkx_edge_labels(semantic_net, pos, edge_labels=edge_labels)\n",
    "    return plt\n",
    "\n",
    "is_a_relationship('Sanatan is a boy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "        words.extend(filtered_tokens)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "    unique_words = set(lemmatized_words)\n",
    "    return (text, unique_words, lemmatized_words, sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, set found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     nx\u001b[39m.\u001b[39mdraw_networkx_edge_labels(semantic_net, pos, edge_labels\u001b[39m=\u001b[39medge_labels)\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m plt\n\u001b[0;32m---> 37\u001b[0m is_a_relationship(\u001b[39m'\u001b[39;49m\u001b[39mSanatan Type boy.\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[96], line 18\u001b[0m, in \u001b[0;36mis_a_relationship\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_a_relationship\u001b[39m(text):\n\u001b[1;32m     17\u001b[0m     semantic_net \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mGraph()\n\u001b[0;32m---> 18\u001b[0m     grams \u001b[39m=\u001b[39m n_grams(text)\n\u001b[1;32m     19\u001b[0m     list_ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis a\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexample\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minstance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mform\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvariety\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvariation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubcategory\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubclass\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubgroup\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubspecies\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubexample\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubinstance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubform\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m     nodes \u001b[39m=\u001b[39m ()\n",
      "Cell \u001b[0;32mIn[96], line 10\u001b[0m, in \u001b[0;36mn_grams\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mn_grams\u001b[39m(text):\n\u001b[1;32m      9\u001b[0m     text \u001b[39m=\u001b[39m preprocess_text(text)\n\u001b[0;32m---> 10\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(text)\n\u001b[1;32m     11\u001b[0m     words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(text)\n\u001b[1;32m     12\u001b[0m     n \u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 1: expected str instance, set found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "import re\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def n_grams(text):\n",
    "    text = preprocess_text(text)\n",
    "    text = ' '.join(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    n =3\n",
    "    ngrams = list(nltk.ngrams(words, n))\n",
    "    return(ngrams)\n",
    "\n",
    "def is_a_relationship(text):\n",
    "    semantic_net = nx.Graph()\n",
    "    grams = n_grams(text)\n",
    "    list_ = ['is', 'is a', 'type', 'kind', 'category', 'class', 'group', 'species', 'example', 'instance', 'form', 'variety', 'model', 'variation', 'subtype', 'subcategory', 'subclass', 'subgroup', 'subspecies', 'subexample', 'subinstance', 'subform']\n",
    "    nodes = ()\n",
    "    edges = ()\n",
    "    for x in grams:\n",
    "        for j in x:\n",
    "            if j in list_:\n",
    "                nodes = nodes + (x[0], x[2], )\n",
    "                edges = edges + ([x[0], x[2], {'relationship': 'is a'}], )\n",
    "                print(edges)\n",
    "    semantic_net.add_nodes_from(nodes)\n",
    "    semantic_net.add_edges_from(edges)\n",
    "    plt.figure()\n",
    "    pos = nx.spring_layout(semantic_net)\n",
    "    nx.draw_networkx(semantic_net, pos, with_labels=True, node_color=\"lightblue\", font_size=10, node_size=1000)\n",
    "    edge_labels = nx.get_edge_attributes(semantic_net, \"relationship\")\n",
    "    nx.draw_networkx_edge_labels(semantic_net, pos, edge_labels=edge_labels)\n",
    "    return plt\n",
    "\n",
    "is_a_relationship('Sanatan Type boy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge_scores(hypotheses, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "def calculate_bleu_score(hypotheses, references):\n",
    "    references = [[ref.split() for ref in references]]\n",
    "    hypotheses = [hyp.split() for hyp in hypotheses]\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "\n",
    "def calculate_meteor_score(hypotheses, references):\n",
    "    meteor = meteor_score(hypotheses, references)\n",
    "    return meteor\n",
    "\n",
    "# Example usage\n",
    "text = \"The code provided is a simplified example to demonstrate the creation and visualization of a semantic network. However, creating a comprehensive and accurate semantic network involves more complex processes and considerations. In the example code, the approach to identifying relationships based on specific keywords can work in some cases. However, it has limitations: Limited coverage: The code focuses on specific keywords like \\\"is a \\\" and \\\"part of.\\\" While these keywords may capture some relationships, they may not cover the entire range of possible semantic relationships. Lack of context: The code analyzes relationships based solely on the presence of specific keywords, without considering the broader context. However, the meaning of relationships can vary depending on the surrounding words and sentence structure.Ambiguity: The code assumes that the second word in the four-gram represents the relationship keyword. However, this assumption may not always hold true, as language is highly contextual and relationships can be expressed in various ways. Creating a comprehensive semantic network typically requires advanced natural language processing (NLP) techniques, such as dependency parsing, named entity recognition, and semantic role labeling. These techniques help extract relationships based on syntactic and semantic analysis of the text. Additionally, domain-specific knowledge and ontologies can be valuable for accurately capturing relationships within a specific field or domain. Overall, while the provided code offers a starting point for understanding the basics of creating a semantic network, building a robust and accurate semantic net often involves more sophisticated techniques and considerations beyond the scope of the example code.\"\n",
    "hypothesis_summaries = [\"The code provided is a simplified example to demonstrate the creation and visualization of a semantic network. However, creating a comprehensive and accurate semantic network involves more complex processes and considerations.\", \"However, the meaning of relationships can vary depending on the surrounding words and sentence structure.Ambiguity: The code assumes that the second word in the four-gram represents the relationship keyword.\"]\n",
    "\n",
    "reference_summaries = [text] * len(hypothesis_summaries)\n",
    "\n",
    "rouge_scores = calculate_rouge_scores(hypothesis_summaries, reference_summaries)\n",
    "bleu_score = calculate_bleu_score(hypothesis_summaries, reference_summaries)\n",
    "meteor_score = calculate_meteor_score(hypothesis_summaries, reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement rouge (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for rouge\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rouge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
